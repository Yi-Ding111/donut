# [DoNut](https://huggingface.co/docs/transformers/main/en/model_doc/donut)

The official tutorial notebook could be found here: [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut)

Qpythonpy





## VisionEncoderDecoderConfig

https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder

Initialize an image-to-text model with 

1. any pretrained Transformer-based vision model as the encoder (*e.g.* [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [BEiT](https://huggingface.co/docs/transformers/model_doc/beit), [DeiT](https://huggingface.co/docs/transformers/model_doc/deit), [Swin](https://huggingface.co/docs/transformers/model_doc/swin)) 
2. any pretrained language model as the decoder (*e.g.* [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta), [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2), [BERT](https://huggingface.co/docs/transformers/model_doc/bert), [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)).



